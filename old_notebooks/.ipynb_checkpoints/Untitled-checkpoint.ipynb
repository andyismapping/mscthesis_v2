{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a1f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import spacepy.datamodel as dm\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import datetime\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy.signal import find_peaks\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from scipy import interpolate\n",
    "import matplotlib.dates as mdates\n",
    "from spacepy import pycdf\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from astropy.time import Time\n",
    "import cdflib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3d9987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARG\n",
    "radar_list = glob.glob('../data/external/Madrigal/2018-2021/*/*.hdf5')\n",
    "radar_list.sort()\n",
    "\n",
    "fileswithne = []\n",
    "for radar in radar_list:\n",
    "    \n",
    "    # metadata\n",
    "    metaA1 = pd.DataFrame(dm.fromHDF5(radar)['Metadata']['Experiment Parameters'])\n",
    "    metaA2 = np.array([x.decode() for x in metaA1['name']])\n",
    "    metaA3 = np.array([x.decode() for x in metaA1['value']])\n",
    "\n",
    "    metaA4 = pd.DataFrame({'name': metaA2,\n",
    "              'value': metaA3})\n",
    "    metaA4.to_csv('../data/processed/Madrigal/{filename}_metadata.csv'.format(filename =Path(radar_list[0]).name))\n",
    "\n",
    "    # variables\n",
    "    varA1 = pd.DataFrame(dm.fromHDF5(radar)['Metadata']['Data Parameters'])\n",
    "    varA2 = np.array([x.decode() for x in varA1['mnemonic']])\n",
    "    varA3 = np.array([x.decode() for x in varA1['description']])\n",
    "    varA4 = np.array([x.decode() for x in varA1['units']])\n",
    "\n",
    "    varA5 = pd.DataFrame({'mnemonic': varA2,\n",
    "                  'description': varA3,\n",
    "                      'units': varA4})\n",
    "    varA5.to_csv('../data/processed/Madrigal/{filename}_variables.csv'.format(filename =Path(radar_list[0]).name))\n",
    "\n",
    "    # append a table with files that have Ne     \n",
    "    if len(varA5[(varA5['mnemonic'] == \"NE\")|(varA5['mnemonic'] == \"NEL\")] ) > 0:\n",
    "        fileswithne.append(radar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a9607",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fileswithne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a74276",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileswithne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0489852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dates\n",
    "gf_list = glob.glob('../data/interim/GRACEFO/KBRNE_relative_v1/dat/*')\n",
    "gf_list.sort()\n",
    "\n",
    "headerlist=['CDF Epoch', 'GPS', 'Latitude', 'Longitude', 'Radius', 'Latitude_QD', 'Longitude_QD','MLT GRACE_1_Position', 'GRACE_2_Position', 'Iono_Corr', 'Distance', 'Relative_Hor_TEC', 'Relative_Ne']\n",
    "\n",
    "for gf_file in gf_list:\n",
    "    gf = pd.read_csv(gf_file, sep='\\s+', header=0,index_col=False, names=headerlist)\n",
    "    print(gf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e58aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Time(gf['CDF Epoch'].values[0], format='cdf_epoch').datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7d1e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open aux files\n",
    "iri_calib = pd.read_csv('../data/interim/GRACEFO/KBRNE_relative_v1/aux_files/GRACE-FO-IRI_CALIB.AUX',  skiprows=[0], sep='\\s+',names = ['cdf-epoch', 'Mean-offset-IRI'])\n",
    "outliers = pd.read_csv('../data/interim/GRACEFO/KBRNE_relative_v1/aux_files/GRACE-FO-OUTLIERS.AUX', skiprows=[0,1,2], sep='\\s+')\n",
    "gap_bias = pd.read_csv('../data/interim/GRACEFO/KBRNE_relative_v1/aux_files/GRACE-GAP-BIAS.ARC', sep='\\s+')\n",
    "\n",
    "## lear how to use the aux files ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee39fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "iri_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f85afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7689e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new arc flag 1\n",
    "\n",
    "plt.plot(gap_bias['CDF_Epoch'][gap_bias['new_ARC']==1], gap_bias['new_ARC'][gap_bias['new_ARC']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d3e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(gap_bias['CDF_Epoch'][gap_bias['new_ARC']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15edc91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_bias[gap_bias['CDF_Epoch'] == 63694811097000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3484fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot radars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find conjunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c1127c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dates\n",
    "gf_list = glob.glob('../data/interim/GRACEFO/KBRNE_relative_v1/dat/*')\n",
    "gf_list.sort()\n",
    "\n",
    "headerlist=['CDF Epoch', 'GPS', 'Latitude', 'Longitude', 'Radius', 'Latitude_QD', 'Longitude_QD','MLT GRACE_1_Position', 'GRACE_2_Position', 'Iono_Corr', 'Distance', 'Relative_Hor_TEC', 'Relative_Ne']\n",
    "\n",
    "grf_list_datetimes =[]\n",
    "for gf_file in gf_list:\n",
    "    gf = pd.read_csv(gf_file, sep='\\s+', header=0,index_col=False, names=headerlist)\n",
    "    grf_list_datetimes.append((Time(gf['CDF Epoch'].values[0], format='cdf_epoch').datetime).strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5273cf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:46<00:00,  4.67s/it]\n"
     ]
    }
   ],
   "source": [
    "radar_list = glob.glob('../data/external/Madrigal/2018-2021/*/*.hdf5')\n",
    "radar_list.sort()\n",
    "\n",
    "date_list = []\n",
    "version_list = []\n",
    "experiment_list = []\n",
    "mlh_pass0_list = []\n",
    "mlh_pass1_list = []\n",
    "mlh_pass2_list = []\n",
    "grf_pass0_list = []\n",
    "grf_pass1_list = []\n",
    "grf_pass2_list = []\n",
    "\n",
    "for i in tqdm(range(150,160)):\n",
    "    metaA1 = pd.DataFrame(dm.fromHDF5(radar_list[i])['Metadata']['Experiment Parameters'])\n",
    "    metaA2 = np.array([x.decode() for x in metaA1['name']])\n",
    "    metaA3 = np.array([x.decode() for x in metaA1['value']])\n",
    "\n",
    "    metaA4 = pd.DataFrame({'name': metaA2,\n",
    "                  'value': metaA3})\n",
    "    radar_date = pd.to_datetime(metaA4[metaA4['name'] == 'start time'].value).dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    try:\n",
    "        # get intersection date\n",
    "        grf_idx = np.intersect1d(grf_list_datetimes,radar_date, return_indices=True)[1][0]\n",
    "\n",
    "        # open files and add datetime as index\n",
    "        ## MLH\n",
    "        radar_file = radar_list[i] \n",
    "        radar = dm.fromHDF5(radar_list[0])\n",
    "        radar_datetimes =[]\n",
    "        for t in iter(range(0,len(dm.fromHDF5(radar_list[0])['Data']['Array Layout']['timestamps']))):\n",
    "\n",
    "            radar_datetimes.append(datetime.datetime.fromtimestamp(dm.fromHDF5(radar_list[0])['Data']['Array Layout']['timestamps'][t])) \n",
    "        \n",
    "        radar['dates'] = radar_datetimes\n",
    "\n",
    "        ## GRACE\n",
    "        gf_file = gf_list[grf_idx]\n",
    "        gf = pd.read_csv(gf_file, sep='\\s+', header=0,index_col=False, names=headerlist)\n",
    "        gf['dates'] = (Time(gf['CDF Epoch'].values, format='cdf_epoch')).datetime\n",
    "\n",
    "        # find GRACE observations that are +/- 5 degrees from radar observations\n",
    "        gf = gf[(gf['Latitude'] <= float(metaA4[metaA4['name'] == 'instrument latitude']['value']) + 5) & (gf['Latitude'] >= float(metaA4[metaA4['name'] == 'instrument latitude']['value']) - 5)]\n",
    "        gf = gf[(gf['Longitude'] <= float(metaA4[metaA4['name'] == 'instrument longitude']['value']) + 5) & (gf['Longitude'] >= float(metaA4[metaA4['name'] == 'instrument longitude']['value']) - 5)]\n",
    "        \n",
    "        if len(gf)>0:\n",
    "#             print('Spatial conjunction on:', mlh_date)\n",
    "\n",
    "            gf_pass = gf[gf.diff(axis = 0)['dates'] > datetime.timedelta(minutes=30)]\n",
    "\n",
    "            if len(gf_pass) == 0:\n",
    "                print('GRF unique passage ', gf['dates'][0], '->', gf['dates'][-1])\n",
    "                time_window_i = gf['dates'][0] - datetime.timedelta(minutes=15)\n",
    "                time_window_f = gf['dates'][-1] + datetime.timedelta(minutes=15)\n",
    "\n",
    "                radar_plot = radar[(radar['dates'] >= time_window_i) & (radar['dates'] <= time_window_f)]\n",
    "                gf_plot = gf[(gf['dates'] >= gf['dates'][0]) & (gf.index <= gf['dates'][-1])]\n",
    "\n",
    "                if len(radar_plot)>0:\n",
    "                    print('Number MLH profiles within time window ', len(np.unique(mlh_plot.index)))\n",
    "                    print('Number GRF observations within time window ', len(grf_plot))\n",
    "                                        \n",
    "                    date_list.append(radar_date.strftime('%Y-%m-%d'))\n",
    "#                     version_list.append(radar_file[38:41])\n",
    "#                     experiment_list.append(mlh_file[42:50])\n",
    "#                     mlh_pass0_list.append(len(np.unique(mlh_plot.index)))\n",
    "#                     grf_pass0_list.append(len(grf_plot))\n",
    "#                     mlh_pass1_list.append(np.nan)\n",
    "#                     grf_pass1_list.append(np.nan)\n",
    "#                     mlh_pass2_list.append(np.nan)\n",
    "#                     grf_pass2_list.append(np.nan)\n",
    "                    \n",
    "#                     grf_plot.to_csv('grf_{date}_v{version}_exp{exp}_pass0.csv'.format(date = mlh_date.strftime('%Y-%m-%d'),\n",
    "#                                                                       version = mlh_file[38:41],\n",
    "#                                                                       exp = mlh_file[42:50]))\n",
    "#                     mlh_plot.to_csv('mlh_{date}_v{version}_exp{exp}_pass0.csv'.format(date = mlh_date.strftime('%Y-%m-%d'),\n",
    "#                                                                       version = mlh_file[38:41],\n",
    "#                                                                       exp = mlh_file[42:50]))\n",
    "                    \n",
    "                    \n",
    "#                 else:\n",
    "#                     print('No MLh profiles within time window')\n",
    "#                     pass\n",
    "\n",
    "#             if len(grf_pass) == 1:\n",
    "#                 pass_idx = np.argwhere(grf.index == grf_pass.index[0])[0][0]\n",
    "                \n",
    "#                 print('GRF first passage', grf.index[0], '->', grf.index[pass_idx-1])\n",
    "#                 time_window_i = grf.index[0] - datetime.timedelta(minutes=15)\n",
    "#                 time_window_f = grf.index[pass_idx-1] + datetime.timedelta(minutes=15)\n",
    "                \n",
    "#                 mlh_plot = mlh[(mlh.index >= time_window_i) & (mlh.index <= time_window_f)]\n",
    "#                 grf_plot = grf[(grf.index >= grf.index[0]) & (grf.index <= grf.index[pass_idx-1])]\n",
    "                \n",
    "#                 if len(mlh_plot)>0:\n",
    "#                     print('Number MLH profiles within time window ', len(np.unique(mlh_plot.index)))\n",
    "#                     print('Number GRF observations within time window ', len(grf_plot))\n",
    "                    \n",
    "#                     date_list.append(mlh_date.strftime('%Y-%m-%d'))\n",
    "#                     version_list.append(mlh_file[38:41])\n",
    "#                     experiment_list.append(mlh_file[42:50])\n",
    "#                     mlh_pass0_list.append(np.nan)\n",
    "#                     grf_pass0_list.append(np.nan)\n",
    "#                     mlh_pass1_list.append(len(np.unique(mlh_plot.index)))\n",
    "#                     grf_pass1_list.append(len(grf_plot))\n",
    "#                     mlh_pass2_list.append(np.nan)\n",
    "#                     grf_pass2_list.append(np.nan)\n",
    "\n",
    "                    \n",
    "#                     grf_plot.to_csv('grf_{date}_v{version}_exp{exp}_pass1.csv'.format(date = mlh_date.strftime('%Y-%m-%d'),\n",
    "#                                                                       version = mlh_file[38:41],\n",
    "#                                                                       exp = mlh_file[42:50]))\n",
    "#                     mlh_plot.to_csv('mlh_{date}_v{version}_exp{exp}_pass1.csv'.format(date = mlh_date.strftime('%Y-%m-%d'),\n",
    "#                                                                       version = mlh_file[38:41],\n",
    "#                                                                       exp = mlh_file[42:50]))\n",
    "#                 else:\n",
    "#                     print('No MLh profiles within time window')\n",
    "#                     pass\n",
    "                \n",
    "                \n",
    "#                 print('GRF second passage', grf.index[pass_idx], '->', grf.index[-1])\n",
    "#                 time_window_i = grf.index[pass_idx] - datetime.timedelta(minutes=15)\n",
    "#                 time_window_f = grf.index[-1] + datetime.timedelta(minutes=15)\n",
    "                \n",
    "#                 mlh_plot = mlh[(mlh.index >= time_window_i) & (mlh.index <= time_window_f)]\n",
    "#                 grf_plot = grf[(grf.index >= grf.index[pass_idx]) & (grf.index <= grf.index[-1])]\n",
    "\n",
    "                \n",
    "#                 if len(mlh_plot)>0:\n",
    "#                     print('Number MLH profiles within time window ', len(np.unique(mlh_plot.index)))\n",
    "#                     print('Number GRF observations within time window ', len(grf_plot))\n",
    "                    \n",
    "#                     date_list.append(mlh_date.strftime('%Y-%m-%d'))\n",
    "#                     version_list.append(mlh_file[38:41])\n",
    "#                     experiment_list.append(mlh_file[42:50])\n",
    "#                     mlh_pass0_list.append(np.nan)\n",
    "#                     grf_pass0_list.append(np.nan)\n",
    "#                     mlh_pass1_list.append(np.nan)\n",
    "#                     grf_pass1_list.append(np.nan)\n",
    "#                     mlh_pass2_list.append(len(np.unique(mlh_plot.index)))\n",
    "#                     grf_pass2_list.append(len(grf_plot))\n",
    "\n",
    "                    \n",
    "#                     grf_plot.to_csv('grf_{date}_v{version}_exp{exp}_pass2.csv'.format(date = mlh_date.strftime('%Y-%m-%d'),\n",
    "#                                                                       version = mlh_file[38:41],\n",
    "#                                                                       exp = mlh_file[42:50]))\n",
    "#                     mlh_plot.to_csv('mlh_{date}_v{version}_exp{exp}_pass2.csv'.format(date = mlh_date.strftime('%Y-%m-%d'),\n",
    "#                                                                       version = mlh_file[38:41],\n",
    "#                                                                       exp = mlh_file[42:50]))\n",
    "                    \n",
    "#                 else:\n",
    "#                     print('No MLh profiles within time window')\n",
    "#                     pass\n",
    "\n",
    "\n",
    "#             print('-------------')\n",
    "            \n",
    "#         else:\n",
    "#             print('No spatial conjunction in : ', mlh_date)\n",
    "#             print('-------------')\n",
    "#             pass\n",
    "\n",
    "    except:\n",
    "#         print('No GRF file in ', mlh_date)\n",
    "#         print('-------------')\n",
    "        pass        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2545e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:41<00:10,  5.21s/it]"
     ]
    }
   ],
   "source": [
    " \n",
    "# for i in tqdm(range(0,len(radar_list))):\n",
    "for i in tqdm(range(150,160)):\n",
    "\n",
    "    metaA1 = pd.DataFrame(dm.fromHDF5(radar_list[i])['Metadata']['Experiment Parameters'])\n",
    "    metaA2 = np.array([x.decode() for x in metaA1['name']])\n",
    "    metaA3 = np.array([x.decode() for x in metaA1['value']])\n",
    "\n",
    "    metaA4 = pd.DataFrame({'name': metaA2,\n",
    "                  'value': metaA3})\n",
    "    radar_date = pd.to_datetime(metaA4[metaA4['name'] == 'start time'].value).dt.strftime('%Y-%m-%d')    \n",
    "    try:\n",
    "        # get intersection date\n",
    "        grf_idx = np.intersect1d(grf_list_datetimes,radar_date, return_indices=True)[1][0]\n",
    "\n",
    "        # open files and add datetime as index\n",
    "        ## MLH\n",
    "        radar_file = radar_list[i] \n",
    "        radar = dm.fromHDF5(radar_list[0])\n",
    "        radar_datetimes =[]\n",
    "        for t in iter(range(0,len(dm.fromHDF5(radar_list[0])['Data']['Array Layout']['timestamps']))):\n",
    "\n",
    "            radar_datetimes.append(datetime.datetime.fromtimestamp(dm.fromHDF5(radar_list[0])['Data']['Array Layout']['timestamps'][t])) \n",
    "        \n",
    "        radar['dates'] = radar_datetimes\n",
    "\n",
    "        ## GRACE\n",
    "        grf_file = gf_list[grf_idx]\n",
    "        grf = pd.read_csv(grf_file, sep='\\s+', header=0,index_col=False, names=headerlist)\n",
    "        grf['dates'] = (Time(grf['CDF Epoch'].values, format='cdf_epoch')).datetime\n",
    "        \n",
    "#         grf_df = pd.DataFrame({\n",
    "#         'datetimes': grf['dates'],\n",
    "#         'ne' : grf['Relative_Ne'],\n",
    "#         'lat': grf['Latitude'],\n",
    "#         'lon' : grf['Longitude'],\n",
    "#         'qd_lat': grf['Latitude_QD'],\n",
    "#         'qd_lon': grf['Longitude_QD'],\n",
    "#         'r': grf['Radius'][...]*0.001\n",
    "#         })\n",
    "        \n",
    "#         grf=grf_df\n",
    "#         print(grf)\n",
    "        \n",
    "        # find GRACE observations that are within spatial window\n",
    "        MLH_lat = float(metaA4[metaA4['name'] == 'instrument latitude']['value'])\n",
    "        MLH_lon = float(metaA4[metaA4['name'] == 'instrument longitude']['value'])\n",
    "        grf = grf[((grf['Latitude'] <= MLH_lat + spatial_window) & (grf['Latitude'] >= MLH_lat - spatial_window))]\n",
    "        grf = grf[((grf.lon <= grf['Longitude'] + spatial_window) & (grf['Longitude'] >= MLH_lon - spatial_window))] \n",
    "        \n",
    "        print('oi')\n",
    "        if len(grf)>0:\n",
    "            grf_pass = grf[grf.diff(axis = 0).datetimes > datetime.timedelta(minutes=temporal_window)]\n",
    "        \n",
    "            if len(grf_pass) == 0: # only one passage\n",
    "\n",
    "                print('GRF passage ', grf.index[0], '->', grf.index[-1])\n",
    "\n",
    "                # define time window\n",
    "                time_window_i = grf.datetimes[grf.index[0]] - datetime.timedelta(minutes=temporal_window)\n",
    "                time_window_f = grf.datetimes[grf.index[-1]] + datetime.timedelta(minutes=temporal_window)\n",
    "                \n",
    "                # filter values\n",
    "                grf_select = grf[(grf.index >= grf.index[0]) & (grf.index <= grf.index[-1])] # within time window\n",
    "\n",
    "                mlh_select = mlh[(mlh.index >= time_window_i) & (mlh.index <= time_window_f)]\n",
    "                \n",
    "                \n",
    "\n",
    "#                 if ((len(mlh_select) > 0) and (len(grf_select) > 0)):\n",
    "\n",
    "#                         grf_select.to_csv('grf_{date}_v{version}_exp{exp}.csv'.format(date = mlh_date.strftime('%Y-%m-%d'),\n",
    "#                                                                           version = mlh_file[38:41],\n",
    "#                                                                           exp = mlh_file[42:50]))\n",
    "#                         mlh_select.to_csv('mlh_{date}_v{version}_exp{exp}.csv'.format(date = mlh_date.strftime('%Y-%m-%d'),\n",
    "#                                                                           version = mlh_file[38:41],\n",
    "#                                                                           exp = mlh_file[42:50]))\n",
    "#                 else:\n",
    "#                     pass\n",
    "\n",
    "\n",
    "            if len(grf_pass) == 1: # two passages\n",
    "\n",
    "                pass_idx = np.argwhere(grf.index == grf_pass.index[0])[0][0]\n",
    "\n",
    "                # first one\n",
    "                print('GRF passage', grf.index[0], '->', grf.index[pass_idx-1])\n",
    "\n",
    "                # define time window\n",
    "                time_window_i = grf.datetimes[grf.index[0]] - datetime.timedelta(minutes=15)\n",
    "                time_window_f = grf.datetimes[grf.index[pass_idx-1]] + datetime.timedelta(minutes=15)\n",
    "\n",
    "                # filter values\n",
    "                grf_select = grf[(grf.index >= grf.index[0]) & (grf.index <= grf.index[pass_idx-1])] # within time window\n",
    "\n",
    "                mlh_select = mlh[(mlh.index >= time_window_i) & (mlh.index <= time_window_f)]\n",
    "\n",
    "#                 if ((len(mlh_select) > 0) and (len(grf_select) > 0)):\n",
    "\n",
    "#                         grf_select.to_csv('grf_{date}_v{version}_exp{exp}.csv'.format(date = mlh_date.strftime('%Y-%m-%d'),\n",
    "#                                                                           version = mlh_file[38:41],\n",
    "#                                                                           exp = mlh_file[42:50]))\n",
    "#                         mlh_select.to_csv('mlh_{date}_v{version}_exp{exp}.csv'.format(date = mlh_date.strftime('%Y-%m-%d'),\n",
    "#                                                                           version = mlh_file[38:41],\n",
    "#                                                                           exp = mlh_file[42:50]))\n",
    "#                 else:\n",
    "#                     pass\n",
    "\n",
    "                # second one\n",
    "                print('GRF passage', grf.index[pass_idx], '->', grf.index[-1])\n",
    "\n",
    "                # define time window\n",
    "                time_window_i = grf.datetimes[grf.index[pass_idx]] - datetime.timedelta(minutes=15)\n",
    "                time_window_f = grf.datetimes[grf.index[-1]] + datetime.timedelta(minutes=15)\n",
    "\n",
    "                # filter values\n",
    "                grf_select = grf[(grf.index >= grf.index[pass_idx]) & (grf.index <= grf.index[-1])] # within time window\n",
    "\n",
    "                mlh_select = mlh[(mlh.index >= time_window_i) & (mlh.index <= time_window_f)]\n",
    "\n",
    "#                 if ((len(mlh_select) > 0) and (len(grf_select) > 0)):\n",
    "\n",
    "#                         grf_select.to_csv('grf_{date}_v{version}_exp{exp}.csv'.format(date = mlh_date.strftime('%Y-%m-%d'),\n",
    "#                                                                           version = mlh_file[38:41],\n",
    "#                                                                           exp = mlh_file[42:50]))\n",
    "#                         mlh_select.to_csv('mlh_{date}_v{version}_exp{exp}.csv'.format(date = mlh_date.strftime('%Y-%m-%d'),\n",
    "#                                                                           version = mlh_file[38:41],\n",
    "#                                                                           exp = mlh_file[42:50]))\n",
    "#                 else:\n",
    "#                     pass\n",
    "        else:\n",
    "            pass\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d163df56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ff2bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef80d097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5bf6be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd5d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d95f9899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dmarray([[7.00000e+05, 6.33403e+10, 6.03664e+10, ..., 4.44310e+09,\n",
       "          4.89110e+09, 4.32810e+09],\n",
       "         [1.06050e+11, 1.09169e+11, 1.06083e+11, ..., 8.85060e+09,\n",
       "          7.12180e+09, 3.94490e+09],\n",
       "         [1.11636e+11, 1.19121e+11, 1.16032e+11, ..., 3.77630e+09,\n",
       "          5.68000e+09, 2.80640e+09],\n",
       "         ...,\n",
       "         [3.41318e+10, 5.72159e+10, 4.95072e+10, ..., 1.15873e+10,\n",
       "          1.27142e+10, 1.27567e+10],\n",
       "         [3.41118e+10, 5.77888e+10, 4.98728e+10, ..., 1.15873e+10,\n",
       "          1.27142e+10, 1.27567e+10],\n",
       "         [3.40942e+10, 5.83566e+10, 5.02411e+10, ..., 1.15873e+10,\n",
       "          1.27142e+10, 1.27567e+10]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dm.fromHDF5(radar_list[0])['Data']['Array Layout']['2D Parameters']['ne']\n",
    "\n",
    "# dm.fromHDF5(radar_list[0])['Data']['Array Layout']['range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f974378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f23a82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(metaA4[metaA4['name'] == 'instrument latitude']['value']) + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90bd0265",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf_file = gf_list[grf_idx]\n",
    "gf = pd.read_csv(gf_file, sep='\\s+', header=0,index_col=False, names=headerlist)\n",
    "# gf['dates'] = (Time(gf['CDF Epoch'].values, format='cdf_epoch')).datetime\n",
    "\n",
    "#         #             gf_pass = gf[gf.diff(axis = 0)['dates'] > datetime.timedelta(minutes=30)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbef1342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CDF Epoch', 'GPS', 'Latitude', 'Longitude', 'Radius', 'Latitude_QD',\n",
       "       'Longitude_QD', 'MLT GRACE_1_Position', 'GRACE_2_Position', 'Iono_Corr',\n",
       "       'Distance', 'Relative_Hor_TEC', 'Relative_Ne'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b6f29f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
